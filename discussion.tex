\section{Discussion}

\subsection{Evaluation Metrics}
In a supervised evaluation framework, given a groundtruth mask $G$ on a particular frame and an output segmentation $M$,
any evaluation measure ultimately has to answerthe question how well $M$ fits $G$. As justified in \cite{pont2016supervised}, 
for images one can use two complementary points of view, regionbased and contour-based measures. As videos extends the
dimensionality of still images to time, the temporal stability of the results must also be considered.

\subsubsection{Accuracy}
\paragraph{Region Similarity $\J$}
To measure the region-based segmentation similarity, i.e. the number of mislabeled pixels,
one employ the Jaccard index $\J$ defined as the intersectionover-union of the estimated segmentation and the groundtruth mask.
The Jaccard index has been widely adopted since its first appearance in PASCAL VOC2008 \cite{martin2004learning}, 
as it provides intuitive, scale-invariant information on the number of mislabeled pixels. Given an output segmentation $M$ and 
the corresponding ground-truth mask $G$ it is defined as $\J = \frac{|M \cap G|}{|M\cup G|}$

\paragraph{Contour Accuracy $\F$}
From a contour-based perspective, one can interpret $M$ as a set of closed contours $c(M)$
delimiting the spatial extent of the mask. Therefore, one
can compute the contour-based precision and recall $Pc$ and
$Rc$ between the contour points of $c(M)$ and $c(G)$, via a bipartite graph matching in order to be robust to small inaccuracies,
as proposed in \cite{martin2004learning}.
So the F-measure $F$ is a good trade-off between two, which is  defined as $F = \frac{2Pc Rc}{Pc+Rc}$.

\paragraph{Temporal Stability $\T$}
Temporal stability $\T$. Intuitively, $\J$ measures how well the pixels of the two masks match, while $F$ measures the
accuracy of the contours. However, temporal stability of the results is a relevant aspect in video object segmentationsince the evolution of object shapes is an important cue for
recognition and jittery, unstable boundaries are unacceptable in video editing applications. 

\subsubsection{Execution Time}

\subsection{Results}

% run the experiment on all datasets

\subsubsection{Single Object Semi-supervised VOS}

\subsubsection{Multiple Objects Semi-supervised VOS}

\subsubsection{Unsupervised VOS}
As shown in Table\ref{table:unsuperivsed_all_dataset}, we can compare the performance of difference algorithm. In generally, the method based on deep learning would excel the the traditional 
methodsï¼ŽThe method combined spatial-temporal information can greatly help improve performance in paper\cite{Tokmakov2017Learning}. And in normal case, optical flow method can help capture the 
temporal information, but it is still not powerful enough. The visual memory module applied in temporal can help improve $4.4\%$ in $\J$ evaluation metrics. Let us see some traditional methods,like
\cite{Koh2017Primary,li2018instance}, they can also gain impressive performance in DAVIS dataset because they can design some specifical feature to model sequences data. Li $et.al$ use instance embedding method
to force the algorithm to learn the instance concepts. So they can get excellent performance in instance segmentation setting.

%  \begin{table*}[t!h]
% 	\begin{center}
% 		\setlength\tabcolsep{3pt}
% 		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
% 			\hline
% 			Dataset& Metrics&NLC~\cite{Faktor2014Video} &CUT~\cite{Keuper2015Motion} &FST~\cite{Papazoglou2013Fast} &SFL~\cite{Cheng2017SegFlow:} &LMP~\cite{Tokmakov2017Learning} &FSEG~\cite{Jain2017FusionSeg}  &LVO~\cite{Tokmakov2017Learning} & ARP~\cite{Koh2017Primary} &IET~\cite{Li2018Instance}\\
% 			\hline
% 			\multirow{2}{*}{DAVIS} &$\J$ Mean &55.1     &55.2                        &55.8                          &67.4                         &69.7                             &71.5                           &75.9                           &76.2                       &78.5 \\
% 			\cline{2-11}
% 			&$\F$ Mean &52.3 &55.2 &51.1 &66.7 & 66.3 & infea &72.1 &70.6 &75.5 \\
% 			\hline
% 			\multirow{2}{*}{SegTrack} &$\J$ Mean &52.3 &55.2 &51.1 &66.7 &65.9 &61.4 & infea &70.6 &75.5 \\
% 			\cline{2-11}
% 			&$\F$ Mean &52.3 &55.2 &51.1 &66.7 &65.9 &infea &infea &70.6 &75.5 \\
% 			\hline

% 			\multirow{2}{*}{Youtube} &$\J$ Mean &52.3 &55.2 &51.1 &66.7 &65.9 &68.57 &infea &70.6 &75.5 \\
% 			\cline{2-11}
% 			&$\F$ Mean &52.3 &55.2 &51.1 &66.7 &65.9 &infea &infea &70.6 &75.5  \\
% 			\hline
% 		\end{tabular}
% 	\end{center}
% 	\caption{The result of unsupervised methods on the Video Objects Segmentation datasets .}
% 	\label{table:unsuperivsed_all_dataset}
% \end{table*}


% \subsection{Summary}


\begin{table*}[t!h]
	\begin{center}
		\setlength\tabcolsep{3pt}
		\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
Dataset& Metrics                   &FSEG~\cite{Jain2017FusionSeg}  &LVO~\cite{Tokmakov2017Learning} &LMP~\cite{Tokmakov2017Learning} & POS~\cite{Koh2017Primary} &IET~\cite{li2018instance}\\
\hline
\multirow{2}{*}{DAVIS} &$\J$ mean  &71.51                           &75.9                           &69.7                            &76.3                          &78.5\\
\cline{2-7}
&$\F$ Mean                         &   --                           &72.1                           &66.3                            &71.1                          &75.5\\
\hline
% \multirow{2}{*}{SegTrack} &$\J$Mean&61.4                            &57.3                           &--                              &80                          &--\\
% \cline{2-7}

% &$\F$ Mean                         &  --                            & --                            &--                              &--                          &--  \\
% \hline
% \multirow{2}{*}{Youtube} &$\J$ Mean&68.57                           & --                            &--                              &--                          &-- \\
% \cline{2-7} 
% &$\F$ Mean                         &  --                            & --                            &--                              &--                          &--   \\
% \hline
\end{tabular}
\end{center}

\caption{The result of unsupervised methods on DAVIS datasets.}
\label{table:unsuperivsed_all_dataset}
\end{table*}



\subsection{Summary}
pass

\subsection{Future Research Directions}
