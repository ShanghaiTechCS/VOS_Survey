\section{Backgrounding}


\subsection{Common Deep Network BackBone}
Deep convolutional neural networks (CNNs) \cite{Lecun2014Backpropagation} have been widely used in light of their successes in many computer vision tasks, such as, face recognition\cite{Schroff2015FaceNet},  object detection\cite{Ren2015Faster}, and image classification\cite{Krizhevsky2012ImageNet}. CNNs use the multi-layer nonlinear function to approximate the mapping function from input to output.

The famous LeNet contains 5 layers. Later as the enhancement in the computing capability of GPUs, deeper CNNs with more convolutional layers, including AlexNet (8 layers) \cite{Krizhevsky2012ImageNet} and VGGNet (16 layers) (16 layers)\cite{Simonyan-VGG} are proposed. Experimental results show that the increase in depth improves the capability of CNNs for image classification. Fig.\ref{VGG} shows the architecture of VGG.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{./figure/VGG.png}
    \caption{VGG Network}
    \label{VGG}
\end{figure}

In GoogLeNet\cite{Szegedy-GoogLeNet}, Szegedy \etal propose the `\emph{Inception module}', which is aimed to find out an optimal local sparse structure in a convolutional network. The Inception architecture vision contains three paths with filters sizes $1 \times 1$, $3 \times 3$ and $5\times 5$, an alternative parallel pooling path, and a combination of all those layers with their output filter banks concatenated into a single output vector forming the input of the next stage.
Such an Inception network is a network consisting of modules of such type stacked upon each other, with occasional max-pooling layers with stride 2 to decrease the resolution of the grid.
Compared with AlexNet \cite{Krizhevsky2012ImageNet}, VGGNet\cite{Simonyan-VGG}, GoogLeNet\cite{Szegedy-GoogLeNet}, ResNet is a more deeper framework with residual learning strategy designed by He \etal in \cite{He-Resnet}. ResNet is neural networks in which each layer consists of a residual module $f_i$ and a skip connection bypassing $f_i$.
In \cite{veit2016residual}, Veit \etal propose a new interpretation of residual networks, which shows residual networks can be seen as a collection of many paths of differing length.
In \cite{xie2017aggregated}, Xie \etal propose a highly modularized network architecture named `\emph{ResNeXt}' and introduces a dimension `\emph{cardinality}' as a factor to the dimensions of depth and width. A module in ResNeXt performs a set of transformations each on a low-dimensional embedding, whose outputs are aggregated by summation and the transformations to be aggregated are all of the same topology.
In \cite{huang2016densely}, Huang \etal design the Dense Convolutional Network (DenseNet), which connects each layer
to every other layer in a feed-forward way. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{./figure/FCN.png}
    \caption{FCN framework}
    \label{FCN}
\end{figure}

\subsection{Common Deep Network Architecture}
\subsubsection{FCN}
Semantic pixel-wise segmentation is an active topic of research. Recently, CNNs also have been introduced to solve segmentation problem. 

In \cite{Long2015Fully}, Long \etal define and detail the space of fully convolutional networks, which take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. A skip architecture is also designed that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. 


\subsubsection{SegNet}
In \cite{SegNet}, Badrinarayanan \etal present a deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet, which consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{./figure/SegNet.png}
    \caption{SegNet framework}
    \label{SegNet}
\end{figure}

\subsubsection{U-Net}
Ronneberger \etal propose U-Net for biomedical image segmentation in \cite{Ronneberger2015U}. The architecture of U-Net consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. 
In the upsampling part, U-Net has also a large number of feature channels, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting path, and yields a u-shaped architecture.

\begin{figure*}
    \begin{center}
        \includegraphics[width=\textwidth]{figure/unet.png}
    \end{center}
    \caption{U-Net framework}
    \label{unet}
\end{figure*}
\subsection{Common Method for Video}

\subsubsection{Optical Flow}
Video object segmentation is challenging due to fast moving objects, deforming shapes, and cluttered backgrounds. optical flow can be used to propagate an object segmentation over time but, unfortunately, flow is often inaccurate, particularly around object boundaries. Such boundaries are precisely where we want our segmentation to be accurate. To obtain accurate segmentation across time.  Tsai \etal propose an efﬁcient algorithm\cite{OFL}  that considers video segmentation and optical ﬂow estimation simultaneously, formulate a principled, multiscale, spatio-temporal objective function that uses optical ﬂow to propagate information between frames. For optical ﬂow estimation, particularly at object boundaries.they show that segmentation results can be used to reﬁne the optical ﬂow, and vice versa, in the proposed object ﬂow algorithm, and can be efﬁciently computed by iterative optimization.


\subsection{Common Method for One-Shot}

\subsubsection{Fine-Tune}
Let us assume that one would like to segment an object in a video, for which the only available piece of information is its foreground/background segmentation in one frame. Intuitively, one could analyze the entity, create a model, and search for it in the rest of the frames. For humans, this very limited amount of information is more than enough, and changes in appearance, shape, occlusions, etc. do not pose a signiﬁcant challenge, because we leverage strong priors: ﬁrst “It is an object,” and then “It is this particular object.” Our method is inspired by this gradual reﬁnement.

We train a Fully Convolutional Neural Network (FCN) for the binary classiﬁcation task of separating the foreground object from the background. We use two successive training steps: First we train on a large variety of objects, ofﬂine, to construct a model that is able to discriminate the general notion of a foreground object, i.e., “It is an object.” Then, at test time, we ﬁne-tune the network for a small number of iterations on the particular instance that we aim to segment, i.e., “It is this particular object.” The overview of our method is illustrated in Figure 2.

\subsubsection{Data Dreaming}
To train the function f one would think of using ground truth data for M t−1 and M t (like [9], [11], [12]), however such data is expensive to annotate and rare. [9] thus trains on a set of 30 videos (∼ 2k frames) and requires the model to transfer across multiple tests sets. [10] side-steps the need for consecutive frames by generating synthetic masks M t−1 from a saliency dataset of ∼ 10k images with their corresponding mask M t . We propose a new data generation strategy to reach better results using only ∼100 individual training frames.

Ideally training data should be as similar as possible to the test data, even subtle differences may affect quality (e.g. training on static images for testing on videos under-performs [62]). To ensure our training data is in-domain, we propose to generate it by synthesizing samples from the provided annotated frame (ﬁrst frame) in each target video. This is akin to “lucid dreaming” as we intentionally “dream” the desired data by creating sample images that are plausible hypothetical future frames of the video. The outcome of this process is a large set of frame pairs in the target domain (2.5k pairs per annotation) with known optical ﬂow and mask annotations, see Figure 5.

\subsubsection{Mask Warp}
We use ﬂow in two complementary ways. First, to obtain a better initial estimate of M t we warp M t−1 using the ﬂow F t : M t = f I (I t , w(M t−1 , F t )); we call this "mask warping". Second, we use ﬂow as a direct source of information about the mask M t . As can be seen in Figure 2, when the object is moving relative to background, the ﬂow magnitude kF t k provides a very reasonable estimate of the mask M t . We thus consider using a convnet speciﬁcally for mask estimation from ﬂow: M t = f F (F t , w(M t−1 , F t )), and merge it with the image-only version by naive averaging