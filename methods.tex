\section{Methods}

\subsection{Semi-supervised VOS}

\subsubsection{OSVOS}

\subsubsection{LucidTracker}


\subsubsection{PML}


\subsubsection{DyeNet}


\subsubsection{CTN}


\subsection{Unsupervised VOS}
Video object segmentation is the task of extracting spatio-temporal regions that correspond to object moving in at
least one frame in the video sequence. In contrast to semi-supervised VOS, unsupervised video object segmentation have 
more challages and is more practical in real world. In real world case, with the limitation of resources and the diversity of scenario,
it is difficult to simulate various outdoor scenario and collect dataset from it in laboratory. So unsupervised video object 
segmentation have more important impact on our real daily life. To better understand to unsupervised setting, here we list some
obvious difference between semi-supervised video segmentation.
\begin{enumerate}
    \item no first frame groundtruth is provided in test phase.
    \item no finetune process in test phase is needed.
\end{enumerate}

In unsupervised VOS tasks, we can regard them as zero-shot video objects segmentation. Because we cannot have any objects priors
in test phase, namely that the objects in testset does not exit in training phase. The task's main challages is that we need to infer
the primary objects which are moving in video frames automatically. The algorithm can discovers the most salient, or primary, objects,
that move against a video's background or display different color statistics. To better capture the moving objects against the background,
the object motion is the critical cue for identify salients objects throught entire video sequences. Next, we will introduce some important
methods which is commonly used in unsupervised VOS.

\subsubsection{Motion in Video Sequences}
In traditional static image semantic segmentation, the apperance information play an import role, which means that 
the performance is enough good if we can extract more reliable apperance features. But in video setting, thera are various
difficult challlenges caused by object moving, which are motion blur and ambigious and occluated. We just rely on apperance information
, which can  fail in this specifical scenario. Motion information can greatly help reduce this ambigious. We can capture more temporal information
to help locate objects in videos frames.

Jain $et.al$ \cite{Jain2017FusionSeg} propose an end-to-end learning framework for segmenting generic objects in video,
which learns to combine appearance and motion information to produce pixel level segmentation masks for all prominent objects.
They design a two-stream fully convolutional neural network which fuses together motion and apperance in a unified framework.

\begin{figure}
    \begin{center}
    \includegraphics[width=0.5\textwidth]{figure/FSEG_NET.png}
    \end{center}
    \caption{FSEG Network}
    \label{FSEG}
\end{figure}

As shown in Fig \ref{FSEG}, the network can take two different inputs, which are raw images and optical flow images respectively.
The motion branch can map the motion into foreground objects, which can greatly capture temporal infomation.
In the last fusion stage, this method does not simply concat two stream feature to get the final prediction.
They design a new fusion strategy to create three independent parallel branches. They apply $1\times1$ convolution to apperance and
motion branch. Finally they apply a layer that thkes the elements-wise maximum to obtain the final prediction. The movivation is that 
an object segmentation prediction is reliable if 1) either apperance or motion model along predicts the object 
segmentation with very strong confidence or 2) their combination together predicts the segmentation with high confidence. 

Besides the two stream strategy, there are still the other fusion strategy to fuse the motion information into apperance 
features to provide additional infomation for building a strong representation of objects that evolves over time. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{./figure/LVO_NET.png}
    \caption{LVO Network}
    \label{LVO}
\end{figure}

Pavel $et.al$\cite{Tokmakov2017Learning} proposed a new network structures. In Fig\ref{LVO}, they use a motion network to extract motion features, which 
take as side information to get the final prediction. The motion network is a pretrained network.

We can improve the performance by combining apperance feature and motion features. However, how can we get good motion
prediction by inputing optical flow images. Pavel $et.al$\cite{LMPV} proposed a encoder-decoder style to estimate the motion 
of video directly.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{./figure/LMP_NET.png}
    \caption{LMP Network}
    \label{LMP}
\end{figure}

In Fig.\ref{LMP}, the input is the optical flow and the output is the moving probability of every single pixel. They use the U-net structure, which
add some shortcut connection to add the coarse information to high level feature, which will greatly help enhance the feature representation power.
After getting the motion map, we can directly use this to predict foreground objects in frames. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{./figure/LMP_results.png}
    \caption{LMP Segmentation Pipeline}
    \label{LMP_results}
\end{figure}

Fig.\ref{LMP_results} is the whole pipeline using motion map to predict the final foreground objects. Each row shows: (a) raw images,
(b) optical flow estimated with LDOF \cite{brox2009large}, (c) output of motion network with LDOF flow as input, (d) objectness map computed with
proposals \cite{pinheiro2016learning}, (e) initial moving object segmentation result, (f) refined result with CRF.

We can observe that mapping motion prediction to final foreground segmentation is a realiable methods. The temporal cue can provide more realiable
feature to help do prediciton. So it is critical to advantage the motion cue.


\subsubsection{Visual Memory in Video Sequences}
Motion cue can greatly help capture temporal information. However, motion just can capture the substantial frames and cannot encode more long range
temporal information over several frames. In reality, the RNN have nature attributes that encoder long time dependency. However, the RNN just can 
receive the vector. So representation power is very limited. So Pavel $et.al$\cite{Tokmakov2017Learning} replace the vector in LSTM by the convolution 
operation, which can greatly improve representation power and is more suitable for video segmentation task.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.35\textwidth]{figure/LVO_CONVRRU.png}
    \caption{convolutional GRU}
    \label{CONVGRU}
\end{figure}

We can see from Fig.\ref{CONVGRU} that they replace the linear combination by convolution operations and a tanh nonlinearity. For frame $t$ in the video
sequence, ConvGRU uses the two stream representation $x_t$ and previous state $h_{t-1}$ to compute the new state $h_t$. The dynamics of this computaiton 
are guided by an update gate $z_t$, a forget gate $r_t$, The states  and the gates are $3$D tensor, and can cahracterizer spatio-temporal pattern in
the video, effectively memorizing which objects move and where they move to. The ConvGRU applies a total of six convolutional opeartions at each time step.
All operation are fully differentiable. So the parameters of the convolution can be trained in an end-to-end fashion wiht back propagation through times.
But there are some drawbacks that the network is memory-comsuming. It's very hard to train. 

In the paper, the author proposed a bidirectional CONVGRU, which can encode motion from the fisrt frame and the last from. This technicial can improve the feature
representation power further. As the expriments shown, the bidirectional CONVGRU bring $5\%$ improvements.

\subsubsection{Hand Craft Methods}

As we all know, deep learning method make huge progress in computer vision. Howerver, there are still a mount of traditional method based on 
hand craft features, which can produce comparable result with deep learning. Sometimes the traditional method is easy to explainable and stable.

Most current methods for unconstrined fg/bg video segmentation are graph-based \cite{Lee2011Key, Papazoglou2013Fast, zhang2013video}. The video
is represented using an Markov Random Field graphical model which consist of a fg/bg data term for each pixel and pairwise
terms between neighboring pixels. The data term is iteratively refined by learning learning color model of the foreground
and background. For computational reasons, the pairwise  terms are typical considered only between adjacent pixels inthe same 
frame and corresponding pixels in adjacent frames(using optical flow).







% \mtodo{Sort by IoU:
% 	% IET~\cite{li2018instance}, 
% 	ARP~\cite{koh2017}, 
% 	LVO~\cite{tokmakov17}, 
% 	FSEG~\cite{jain2017}, 
% 	LMP~\cite{tokmakov2017}, 
% 	SFL~\cite{cheng2017sfl}, 
% 	FST~\cite{papazoglou2013}, 
% 	CUT~\cite{keuper2015}, 
% 	NLC~\cite{faktor2014}, 
% 	MSG~\cite{ochs2011}, 
% 	KEY~\cite{lee2011}, 
% 	CVOS~\cite{taylor2015},
% 	TRC~\cite{fragkiadaki2012}
% }
